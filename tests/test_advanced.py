#!/usr/bin/env python3
"""
Advanced Test Suite for Copilot Ralph Mode
=========================================

Production-grade tests using:
- pytest: Modern test framework
- hypothesis: Property-based testing for edge cases
- pytest-parametrize: Data-driven testing
- unittest.mock: Mocking and patching
- pytest-benchmark: Performance testing
- pytest-timeout: Timeout handling

This suite covers:
- Edge cases and boundary conditions
- Concurrency and race conditions
- File system edge cases
- Unicode and encoding
- Security considerations
- Performance benchmarks
- Error handling and recovery
"""

import json
import os
import re
import shutil
import sys
import tempfile
import threading
import time
import unittest
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timezone
from io import StringIO
from pathlib import Path
from unittest import mock

import pytest

# Property-based testing
try:
    from hypothesis import given, settings, strategies as st, assume, example
    HAS_HYPOTHESIS = True
except ImportError:
    HAS_HYPOTHESIS = False
    # Provide dummy decorators
    def given(*args, **kwargs):
        def decorator(f):
            return pytest.mark.skip(reason="hypothesis not installed")(f)
        return decorator
    def settings(*args, **kwargs):
        def decorator(f):
            return f
        return decorator
    def example(*args, **kwargs):
        def decorator(f):
            return f
        return decorator
    class st:
        @staticmethod
        def text(*args, **kwargs):
            return None
        @staticmethod
        def integers(*args, **kwargs):
            return None
        @staticmethod
        def booleans():
            return None
        @staticmethod
        def lists(*args, **kwargs):
            return None
        @staticmethod
        def one_of(*args, **kwargs):
            return None
        @staticmethod
        def none():
            return None
    def assume(x):
        return x

# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from ralph_mode import RalphMode, TaskLibrary, VERSION


# =============================================================================
# Fixtures
# =============================================================================

@pytest.fixture
def temp_dir():
    """Create a temporary directory for each test."""
    d = Path(tempfile.mkdtemp())
    yield d
    shutil.rmtree(d, ignore_errors=True)


@pytest.fixture
def ralph(temp_dir):
    """Create a RalphMode instance with temp directory."""
    return RalphMode(temp_dir)


@pytest.fixture
def active_ralph(ralph):
    """Create an active RalphMode instance."""
    ralph.enable("Test task")
    return ralph


@pytest.fixture
def task_manager(temp_dir):
    """Create a TaskLibrary instance with test tasks."""
    tasks_dir = temp_dir / "tasks"
    tasks_dir.mkdir()
    
    # Create test tasks
    (tasks_dir / "test-task.md").write_text("""---
id: TEST-001
title: Test Task
priority: high
tags:
  - test
  - unit
---

# Test Task

This is a test task for unit testing.
""", encoding='utf-8')
    
    return TaskLibrary(temp_dir)


# =============================================================================
# Property-Based Testing with Hypothesis
# =============================================================================

class TestPropertyBased:
    """Property-based tests using Hypothesis."""
    
    @pytest.fixture(autouse=True)
    def setup(self, temp_dir):
        self.temp_dir = temp_dir
        self.ralph = RalphMode(temp_dir)
    
    @given(st.text(min_size=1, max_size=10000, alphabet=st.characters(
        blacklist_characters='\r',  # Windows normalizes \r to \n in text mode
        blacklist_categories=('Cs',)  # Exclude surrogates
    )))
    @settings(max_examples=100, deadline=None)
    @example("Simple prompt")
    @example("Multi\nline\nprompt")
    @example("Unicode: üöÄ √©mojis ‰∏≠Êñá ÿßŸÑÿπÿ±ÿ®Ÿäÿ©")
    @example("Special: <script>alert('xss')</script>")
    @example("Nested: <promise><promise>test</promise></promise>")
    def test_prompt_storage_roundtrip(self, prompt):
        """Any valid prompt should be stored and retrieved exactly."""
        assume(len(prompt.strip()) > 0)
        
        # Reset for each test
        if self.ralph.is_active():
            self.ralph.disable()
        
        self.ralph.enable(prompt)
        retrieved = self.ralph.get_prompt()
        
        assert retrieved == prompt, f"Prompt mismatch: {repr(prompt)} != {repr(retrieved)}"
        self.ralph.disable()
    
    @given(st.integers(min_value=1, max_value=20))  # Reduced max for performance
    @settings(max_examples=20, deadline=None)
    @example(1)
    @example(5)
    @example(10)
    def test_max_iterations_boundary(self, max_iter):
        """Max iterations should be respected exactly."""
        if self.ralph.is_active():
            self.ralph.disable()
        
        self.ralph.enable("Test", max_iterations=max_iter)
        
        state = self.ralph.get_state()
        assert state['max_iterations'] == max_iter
        
        # Should allow exactly max_iter-1 iterations after enable
        for i in range(max_iter - 1):
            state = self.ralph.iterate()
            assert state['iteration'] == i + 2
        
        # Next iteration should fail
        with pytest.raises(ValueError, match="Max iterations"):
            self.ralph.iterate()
    
    @given(st.text(min_size=1, max_size=500))
    @settings(max_examples=50, deadline=None)
    @example("DONE")
    @example("Task Complete!")
    @example("‚úÖ All tests pass")
    @example("<html>")
    @example("</promise>")
    def test_completion_promise_detection(self, promise):
        """Completion promise detection should be exact match."""
        assume(len(promise.strip()) > 0)
        assume("<promise>" not in promise and "</promise>" not in promise)
        
        if self.ralph.is_active():
            self.ralph.disable()
        
        self.ralph.enable("Test", completion_promise=promise)
        
        # Wrong promise should not match
        assert not self.ralph.check_completion(f"<promise>WRONG_{promise}</promise>")
        
        # Correct promise should match
        assert self.ralph.check_completion(f"<promise>{promise}</promise>")
        
        # Promise in text should match
        assert self.ralph.check_completion(f"Output: <promise>{promise}</promise> done")
        
        self.ralph.disable()


# =============================================================================
# Parametrized Tests
# =============================================================================

class TestParametrized:
    """Data-driven parametrized tests."""
    
    @pytest.fixture(autouse=True)
    def setup(self, temp_dir):
        self.temp_dir = temp_dir
        self.ralph = RalphMode(temp_dir)
    
    @pytest.mark.parametrize("prompt,expected_valid", [
        ("Simple task", True),
        ("", False),
        ("   ", False),
        ("\n\n\n", False),
        ("Task with\nmultiple\nlines", True),
        ("A" * 10000, True),  # Long prompt (10K chars)
        ("Unicode: Êó•Êú¨Ë™û ÌïúÍµ≠Ïñ¥ ÿßŸÑÿπÿ±ÿ®Ÿäÿ©", True),
        ("Emoji: üéâüöÄüí°", True),
        ("Special: <>&\"'", True),
        ("Path: C:\\Users\\test", True),
        ("URL: https://example.com?foo=bar&baz=qux", True),
    ], ids=lambda x: str(x)[:50] if isinstance(x, str) else str(x))
    def test_prompt_validation(self, prompt, expected_valid):
        """Test various prompt inputs."""
        if expected_valid:
            state = self.ralph.enable(prompt)
            assert state is not None
            assert self.ralph.get_prompt() == prompt
            self.ralph.disable()
        else:
            # Empty prompts should still be accepted (no validation in current impl)
            # but let's verify behavior is consistent
            state = self.ralph.enable(prompt)
            assert self.ralph.is_active()
            self.ralph.disable()
    
    @pytest.mark.parametrize("max_iter", [0, 1, 5, 10, 100, 1000])
    def test_max_iterations_values(self, max_iter):
        """Test various max_iterations values."""
        state = self.ralph.enable("Test", max_iterations=max_iter)
        assert state['max_iterations'] == max_iter
        self.ralph.disable()
    
    @pytest.mark.parametrize("promise,output,should_match", [
        ("DONE", "<promise>DONE</promise>", True),
        ("DONE", "<promise>done</promise>", False),  # Case sensitive
        ("DONE", "DONE", False),  # No tags
        ("DONE", "<promise>DONE", False),  # Missing closing tag
        ("DONE", "DONE</promise>", False),  # Missing opening tag
        ("", "<promise></promise>", False),  # Empty promise - no match when promise is empty string
        ("A B C", "<promise>A B C</promise>", True),  # Spaces
        ("Line1\nLine2", "<promise>Line1\nLine2</promise>", True),  # Newlines
        ("100%", "<promise>100%</promise>", True),  # Special chars
        ("<tag>", "<promise><tag></promise>", True),  # Nested tags
    ])
    def test_completion_promise_patterns(self, promise, output, should_match):
        """Test completion promise pattern matching."""
        if promise:  # Only test with non-empty promise
            self.ralph.enable("Test", completion_promise=promise)
            assert self.ralph.check_completion(output) == should_match
            self.ralph.disable()
    
    @pytest.mark.parametrize("num_tasks", [1, 2, 5, 10, 50])
    def test_batch_mode_task_counts(self, num_tasks):
        """Test batch mode with various task counts."""
        tasks = [
            {"id": f"TASK-{i:03d}", "title": f"Task {i}", "prompt": f"Do task {i}"}
            for i in range(num_tasks)
        ]
        
        state = self.ralph.init_batch(tasks, max_iterations=5)
        assert state['tasks_total'] == num_tasks
        assert state['current_task_index'] == 0
        
        self.ralph.disable()


# =============================================================================
# Edge Cases and Boundary Conditions
# =============================================================================

class TestEdgeCases:
    """Tests for edge cases and boundary conditions."""
    
    @pytest.fixture(autouse=True)
    def setup(self, temp_dir):
        self.temp_dir = temp_dir
        self.ralph = RalphMode(temp_dir)
    
    def test_rapid_enable_disable_cycles(self):
        """Test rapid enable/disable cycling doesn't corrupt state."""
        for i in range(100):
            self.ralph.enable(f"Task {i}")
            assert self.ralph.is_active()
            state = self.ralph.get_state()
            assert state['iteration'] == 1
            self.ralph.disable()
            assert not self.ralph.is_active()
    
    def test_very_long_prompt(self):
        """Test handling of very long prompts (1MB+)."""
        long_prompt = "A" * (1024 * 1024)  # 1MB prompt
        
        self.ralph.enable(long_prompt)
        retrieved = self.ralph.get_prompt()
        
        assert len(retrieved) == len(long_prompt)
        assert retrieved == long_prompt
        self.ralph.disable()
    
    def test_deeply_nested_promise_tags(self):
        """Test deeply nested promise tags."""
        self.ralph.enable("Test", completion_promise="DONE")
        
        # Deeply nested tags - regex finds innermost <promise>DONE</promise>
        # but the actual text is "<promise>DONE" not "DONE"
        nested = "<promise><promise><promise>DONE</promise></promise></promise>"
        # The regex will find "<promise>DONE" as the first match content, which won't match "DONE"
        # This is expected behavior - only simple <promise>DONE</promise> should match
        assert not self.ralph.check_completion(nested)
        
        # But simple tag should work
        assert self.ralph.check_completion("<promise>DONE</promise>")
        
        self.ralph.disable()
    
    def test_promise_with_special_regex_chars(self):
        """Test promise with characters special to regex."""
        special_promises = [
            ".*+?^${}()|[]\\",
            "[0-9]+",
            "^start$",
            "a|b|c",
            "\\d+\\.\\d+",
        ]
        
        for promise in special_promises:
            if self.ralph.is_active():
                self.ralph.disable()
            
            self.ralph.enable("Test", completion_promise=promise)
            
            # Should match exact promise
            assert self.ralph.check_completion(f"<promise>{promise}</promise>")
            
            # Should not match similar but different
            assert not self.ralph.check_completion("<promise>different</promise>")
            
            self.ralph.disable()
    
    def test_iteration_at_exact_max(self):
        """Test behavior at exactly max_iterations."""
        self.ralph.enable("Test", max_iterations=3)
        # Start at iteration 1
        
        self.ralph.iterate()  # Now at 2
        self.ralph.iterate()  # Now at 3, which equals max
        
        # Next iteration should hit max and raise (3 >= 3)
        with pytest.raises(ValueError, match="Max iterations"):
            self.ralph.iterate()
        
        # Should be disabled now
        assert not self.ralph.is_active()
    
    def test_batch_single_task(self):
        """Test batch mode with single task."""
        tasks = [{"id": "SINGLE", "title": "Only Task", "prompt": "Do it"}]
        
        state = self.ralph.init_batch(tasks, completion_promise="DONE")
        assert state['tasks_total'] == 1
        
        # Complete the only task
        with pytest.raises(ValueError, match="All tasks completed"):
            self.ralph.next_task(reason="completed")
        
        assert not self.ralph.is_active()
    
    def test_history_with_many_entries(self):
        """Test history handling with many entries."""
        self.ralph.enable("Test", max_iterations=0)  # Unlimited
        
        # Generate many iterations
        for i in range(500):
            self.ralph.iterate()
        
        history = self.ralph.get_history()
        assert len(history) == 501  # 1 start + 500 iterations
        
        # Verify order
        assert history[0]['status'] == 'started'
        assert all(h['status'] == 'iterate' for h in history[1:])
        
        self.ralph.disable()
    
    def test_state_persistence_after_crash_simulation(self):
        """Test state can be recovered after simulated crash."""
        self.ralph.enable("Important task", max_iterations=10, completion_promise="DONE")
        
        for _ in range(5):
            self.ralph.iterate()
        
        # Simulate crash by creating new instance
        ralph2 = RalphMode(self.temp_dir)
        
        assert ralph2.is_active()
        state = ralph2.get_state()
        assert state['iteration'] == 6
        assert state['completion_promise'] == "DONE"
        
        ralph2.disable()
    
    def test_concurrent_file_access(self):
        """Test concurrent access doesn't corrupt state."""
        self.ralph.enable("Test", max_iterations=0)
        
        errors = []
        
        def iterate_thread(n):
            try:
                for _ in range(n):
                    self.ralph.iterate()
            except Exception as e:
                errors.append(str(e))
        
        # Note: This tests the current non-thread-safe implementation
        # In production, you'd want file locking
        threads = [threading.Thread(target=iterate_thread, args=(10,)) for _ in range(5)]
        
        for t in threads:
            t.start()
        for t in threads:
            t.join()
        
        # State should still be readable
        state = self.ralph.get_state()
        assert state is not None
        assert state['iteration'] > 1
        
        self.ralph.disable()


# =============================================================================
# File System Edge Cases
# =============================================================================

class TestFileSystem:
    """Tests for file system edge cases."""
    
    @pytest.fixture(autouse=True)
    def setup(self, temp_dir):
        self.temp_dir = temp_dir
        self.ralph = RalphMode(temp_dir)
    
    def test_path_with_spaces(self):
        """Test paths with spaces work correctly."""
        space_dir = self.temp_dir / "path with spaces"
        space_dir.mkdir()
        
        ralph = RalphMode(space_dir)
        ralph.enable("Test")
        
        assert ralph.is_active()
        assert ralph.ralph_dir.exists()
        
        ralph.disable()
    
    def test_path_with_unicode(self):
        """Test paths with unicode characters."""
        unicode_dir = self.temp_dir / "„ÉÜ„Çπ„Éà_—Ç–µ—Å—Ç_ÊµãËØï"
        unicode_dir.mkdir()
        
        ralph = RalphMode(unicode_dir)
        ralph.enable("Test")
        
        assert ralph.is_active()
        
        ralph.disable()
    
    def test_readonly_state_file(self):
        """Test handling of read-only state file."""
        self.ralph.enable("Test")
        
        # Make state file read-only
        if sys.platform != 'win32':  # Skip on Windows
            os.chmod(self.ralph.state_file, 0o444)
            
            with pytest.raises(PermissionError):
                self.ralph.iterate()
            
            # Restore permissions for cleanup
            os.chmod(self.ralph.state_file, 0o644)
        
        self.ralph.disable()
    
    def test_corrupted_state_file(self):
        """Test handling of corrupted state file."""
        self.ralph.enable("Test")
        
        # Corrupt the state file
        self.ralph.state_file.write_text("not valid json {{{", encoding='utf-8')
        
        # Should handle gracefully
        state = self.ralph.get_state()
        # Depending on implementation, might return None or raise
        # Current impl returns None on parse error
        assert state is None or isinstance(state, dict)
    
    def test_missing_ralph_dir(self):
        """Test behavior when .ralph-mode dir is manually deleted."""
        self.ralph.enable("Test")
        
        # Manually delete the directory
        shutil.rmtree(self.ralph.ralph_dir)
        
        assert not self.ralph.is_active()
        assert self.ralph.get_state() is None
    
    def test_symlink_in_path(self):
        """Test handling of symlinks in path."""
        if sys.platform == 'win32':
            pytest.skip("Symlinks require admin on Windows")
        
        real_dir = self.temp_dir / "real"
        real_dir.mkdir()
        
        link_dir = self.temp_dir / "link"
        link_dir.symlink_to(real_dir)
        
        ralph = RalphMode(link_dir)
        ralph.enable("Test")
        
        assert ralph.is_active()
        
        ralph.disable()


# =============================================================================
# Security Tests
# =============================================================================

class TestSecurity:
    """Security-focused tests."""
    
    @pytest.fixture(autouse=True)
    def setup(self, temp_dir):
        self.temp_dir = temp_dir
        self.ralph = RalphMode(temp_dir)
    
    def test_path_traversal_in_prompt(self):
        """Test path traversal attempts in prompt don't escape."""
        malicious_prompt = "../../../../etc/passwd"
        
        self.ralph.enable(malicious_prompt)
        
        # Should be stored as-is, not interpreted as path
        assert self.ralph.get_prompt() == malicious_prompt
        
        # Verify no files created outside temp_dir
        assert not Path("/etc/passwd").exists() or True  # File exists is unrelated
        
        self.ralph.disable()
    
    def test_command_injection_in_promise(self):
        """Test command injection in completion promise."""
        malicious_promise = "$(rm -rf /); DONE"
        
        self.ralph.enable("Test", completion_promise=malicious_promise)
        
        # Should match literally, not execute
        assert self.ralph.check_completion(f"<promise>{malicious_promise}</promise>")
        
        self.ralph.disable()
    
    def test_json_injection_in_state(self):
        """Test JSON injection attempts in state."""
        # The state is JSON, so special chars should be escaped
        malicious_prompt = '{"injected": true, "original": false}'
        
        self.ralph.enable(malicious_prompt)
        
        state = self.ralph.get_state()
        # State should have original fields, injection should be in prompt only
        assert 'iteration' in state
        assert 'max_iterations' in state
        assert self.ralph.get_prompt() == malicious_prompt
        
        self.ralph.disable()
    
    def test_xss_in_prompt(self):
        """Test XSS payloads are stored verbatim."""
        xss_payloads = [
            "<script>alert('XSS')</script>",
            "<img src=x onerror=alert('XSS')>",
            "javascript:alert('XSS')",
            "<svg onload=alert('XSS')>",
        ]
        
        for payload in xss_payloads:
            if self.ralph.is_active():
                self.ralph.disable()
            
            self.ralph.enable(payload)
            
            # Should be stored exactly as-is
            assert self.ralph.get_prompt() == payload
            
            self.ralph.disable()


# =============================================================================
# TaskLibrary Tests
# =============================================================================

class TestTaskLibrary:
    """Tests for TaskLibrary class."""
    
    @pytest.fixture(autouse=True)
    def setup(self, temp_dir):
        self.temp_dir = temp_dir
        self.tasks_dir = temp_dir / "tasks"
        self.tasks_dir.mkdir()
        self.groups_dir = self.tasks_dir / "_groups"
        self.groups_dir.mkdir()
    
    def _create_task(self, filename: str, content: str):
        """Helper to create a task file."""
        (self.tasks_dir / filename).write_text(content, encoding='utf-8')
    
    def _create_group(self, name: str, tasks: list):
        """Helper to create a group file."""
        (self.groups_dir / f"{name}.json").write_text(
            json.dumps({"name": name, "tasks": tasks}),
            encoding='utf-8'
        )
    
    def test_parse_valid_task_file(self):
        """Test parsing a valid task file."""
        # Use inline array format that the simple parser supports
        self._create_task("test.md", """---
id: TEST-001
title: Test Task
priority: high
tags: [unit, test]
---

# Test Task

Content here.
""")
        
        tm = TaskLibrary(self.temp_dir)
        task = tm.get_task("TEST-001")
        
        assert task is not None
        assert task['id'] == "TEST-001"
        assert task['title'] == "Test Task"
        assert task.get('priority') == "high"
        tags = task.get('tags', [])
        assert 'unit' in tags or 'test' in tags
    
    def test_parse_task_without_frontmatter(self):
        """Test parsing task file without YAML frontmatter."""
        self._create_task("no-front.md", """# Just a Heading

Some content without frontmatter.
""")
        
        tm = TaskLibrary(self.temp_dir)
        tasks = tm.list_tasks()
        
        # Should parse without error, using defaults
        assert len(tasks) >= 0
    
    def test_parse_task_with_malformed_yaml(self):
        """Test handling of malformed YAML in frontmatter."""
        self._create_task("bad-yaml.md", """---
id: TEST-001
title: "unclosed string
priority: [invalid
---

Content.
""")
        
        tm = TaskLibrary(self.temp_dir)
        # Should handle gracefully
        tasks = tm.list_tasks()
        assert isinstance(tasks, list)
    
    def test_list_tasks_empty_directory(self):
        """Test listing tasks in empty directory."""
        empty_dir = self.temp_dir / "empty"
        empty_dir.mkdir()
        (empty_dir / "tasks").mkdir()
        
        tm = TaskLibrary(empty_dir)
        tasks = tm.list_tasks()
        
        assert tasks == []
    
    def test_list_groups(self):
        """Test listing task groups."""
        self._create_group("backend", ["TASK-001", "TASK-002"])
        self._create_group("frontend", ["TASK-003"])
        
        tm = TaskLibrary(self.temp_dir)
        groups = tm.list_groups()
        
        assert len(groups) == 2
        names = [g['name'] for g in groups]
        assert 'backend' in names
        assert 'frontend' in names
    
    def test_search_tasks(self):
        """Test searching tasks."""
        self._create_task("api.md", """---
id: API-001
title: Build REST API
tags:
  - backend
---

Implement the REST API.
""")
        
        self._create_task("ui.md", """---
id: UI-001
title: Build React UI
tags:
  - frontend
---

Create the React frontend.
""")
        
        tm = TaskLibrary(self.temp_dir)
        
        # Search by title
        results = tm.search_tasks("REST")
        assert len(results) == 1
        assert results[0]['id'] == "API-001"
        
        # Search by tag
        results = tm.search_tasks("frontend")
        assert len(results) == 1
        assert results[0]['id'] == "UI-001"
    
    def test_get_nonexistent_task(self):
        """Test getting a task that doesn't exist."""
        tm = TaskLibrary(self.temp_dir)
        task = tm.get_task("NONEXISTENT-999")
        
        assert task is None
    
    def test_get_nonexistent_group(self):
        """Test getting a group that doesn't exist."""
        tm = TaskLibrary(self.temp_dir)
        group = tm.get_group("nonexistent")
        
        assert group is None


# =============================================================================
# Performance Tests
# =============================================================================

class TestPerformance:
    """Performance and stress tests."""
    
    @pytest.fixture(autouse=True)
    def setup(self, temp_dir):
        self.temp_dir = temp_dir
        self.ralph = RalphMode(temp_dir)
    
    @pytest.mark.timeout(10)
    def test_enable_disable_performance(self):
        """Test enable/disable operations complete within timeout."""
        start = time.time()
        
        for _ in range(50):
            self.ralph.enable("Test")
            self.ralph.disable()
        
        elapsed = time.time() - start
        assert elapsed < 5.0, f"50 enable/disable cycles took {elapsed:.2f}s"
    
    @pytest.mark.timeout(10)
    def test_iteration_performance(self):
        """Test iteration operations complete within timeout."""
        self.ralph.enable("Test", max_iterations=0)
        
        start = time.time()
        
        for _ in range(200):
            self.ralph.iterate()
        
        elapsed = time.time() - start
        assert elapsed < 5.0, f"200 iterations took {elapsed:.2f}s"
        
        self.ralph.disable()
    
    @pytest.mark.timeout(10)
    def test_history_read_performance(self):
        """Test history reading performance with many entries."""
        self.ralph.enable("Test", max_iterations=0)
        
        # Generate many history entries
        for _ in range(500):
            self.ralph.iterate()
        
        start = time.time()
        
        for _ in range(100):
            history = self.ralph.get_history()
        
        elapsed = time.time() - start
        assert elapsed < 3.0, f"100 history reads took {elapsed:.2f}s"
        
        self.ralph.disable()
    
    @pytest.mark.timeout(30)
    def test_large_batch_performance(self):
        """Test batch mode with many tasks."""
        tasks = [
            {"id": f"TASK-{i:04d}", "title": f"Task {i}", "prompt": f"Do task {i}"}
            for i in range(100)
        ]
        
        start = time.time()
        
        self.ralph.init_batch(tasks, max_iterations=1, completion_promise="DONE")
        
        # Complete all tasks via max iterations
        for i in range(99):  # 100 tasks, first one starts at index 0
            try:
                self.ralph.iterate()
            except ValueError:
                pass  # All tasks completed
        
        elapsed = time.time() - start
        assert elapsed < 20.0, f"100 task batch took {elapsed:.2f}s"


# =============================================================================
# Integration Tests
# =============================================================================

class TestIntegration:
    """Integration tests for complete workflows."""
    
    @pytest.fixture(autouse=True)
    def setup(self, temp_dir):
        self.temp_dir = temp_dir
        self.ralph = RalphMode(temp_dir)
    
    def test_full_single_task_workflow(self):
        """Test complete single task workflow."""
        # Enable
        state = self.ralph.enable(
            prompt="Build a REST API with authentication",
            max_iterations=10,
            completion_promise="API_COMPLETE"
        )
        
        assert state['iteration'] == 1
        assert state['mode'] == 'single'
        
        # Simulate iterations
        for i in range(3):
            state = self.ralph.iterate()
        
        assert state['iteration'] == 4
        
        # Check status
        status = self.ralph.status()
        assert status['iteration'] == 4
        assert status['history_entries'] == 4
        
        # Complete
        result = self.ralph.complete("Output <promise>API_COMPLETE</promise>")
        assert result is True
        assert not self.ralph.is_active()
    
    def test_full_batch_workflow(self):
        """Test complete batch task workflow."""
        tasks = [
            {"id": "BE-001", "title": "Backend API", "prompt": "Build API"},
            {"id": "FE-001", "title": "Frontend UI", "prompt": "Build UI"},
            {"id": "TEST-001", "title": "Testing", "prompt": "Add tests"},
        ]
        
        # Init batch
        state = self.ralph.init_batch(
            tasks=tasks,
            max_iterations=5,
            completion_promise="DONE"
        )
        
        assert state['mode'] == 'batch'
        assert state['tasks_total'] == 3
        assert state['current_task_index'] == 0
        
        # Complete first task
        self.ralph.complete("<promise>DONE</promise>")
        state = self.ralph.get_state()
        assert state['current_task_index'] == 1
        
        # Complete second task via next_task
        state = self.ralph.next_task(reason="completed manually")
        assert state['current_task_index'] == 2
        
        # Complete final task
        with pytest.raises(ValueError, match="All tasks completed"):
            self.ralph.next_task(reason="completed")
        
        assert not self.ralph.is_active()
    
    def test_auto_agents_workflow(self):
        """Test workflow with auto-agents enabled."""
        state = self.ralph.enable(
            prompt="Build complex system",
            auto_agents=True
        )
        
        assert state['auto_agents'] is True
        assert state['created_agents'] == []
        
        # Register agents during iterations
        self.ralph.register_created_agent("test-agent", ".github/agents/test-agent.agent.md")
        self.ralph.register_created_agent("review-agent", ".github/agents/review-agent.agent.md")
        
        state = self.ralph.get_state()
        assert len(state['created_agents']) == 2
        
        # Check instructions include auto-agents section
        instructions = self.ralph.instructions_file.read_text(encoding='utf-8')
        assert "Auto-Agents" in instructions
        
        self.ralph.disable()
    
    def test_recovery_workflow(self):
        """Test recovery after simulated interruption."""
        # Start work
        self.ralph.enable("Important task", max_iterations=20)
        
        for _ in range(5):
            self.ralph.iterate()
        
        # Save state
        original_state = self.ralph.get_state()
        
        # Simulate restart (new instance)
        ralph2 = RalphMode(self.temp_dir)
        
        # Verify state recovered
        recovered_state = ralph2.get_state()
        assert recovered_state['iteration'] == original_state['iteration']
        
        # Continue work
        ralph2.iterate()
        
        final_state = ralph2.get_state()
        assert final_state['iteration'] == 7
        
        ralph2.disable()


# =============================================================================
# Error Handling Tests
# =============================================================================

class TestErrorHandling:
    """Tests for error handling and recovery."""
    
    @pytest.fixture(autouse=True)
    def setup(self, temp_dir):
        self.temp_dir = temp_dir
        self.ralph = RalphMode(temp_dir)
    
    def test_iterate_when_inactive_raises(self):
        """Test iterate raises when not active."""
        with pytest.raises(ValueError, match="(not active|No active)"):
            self.ralph.iterate()
    
    def test_enable_when_active_raises(self):
        """Test enable raises when already active."""
        self.ralph.enable("First")
        
        with pytest.raises(ValueError, match="already active"):
            self.ralph.enable("Second")
    
    def test_next_task_in_single_mode_raises(self):
        """Test next_task raises in single mode."""
        self.ralph.enable("Single task")
        
        with pytest.raises(ValueError, match="batch mode"):
            self.ralph.next_task(reason="test")
    
    def test_disable_when_inactive_returns_none(self):
        """Test disable returns None when not active."""
        result = self.ralph.disable()
        assert result is None
    
    def test_status_when_inactive_returns_none(self):
        """Test status returns None when not active."""
        result = self.ralph.status()
        assert result is None
    
    def test_get_prompt_when_inactive_returns_none(self):
        """Test get_prompt returns None when not active."""
        result = self.ralph.get_prompt()
        assert result is None
    
    def test_complete_when_inactive(self):
        """Test complete behavior when not active."""
        result = self.ralph.complete("Some output")
        assert result is False
    
    def test_check_completion_without_promise(self):
        """Test check_completion when no promise set."""
        self.ralph.enable("Test")  # No completion_promise
        
        # Should return False since no promise to match
        result = self.ralph.check_completion("<promise>ANYTHING</promise>")
        assert result is False
        
        self.ralph.disable()


# =============================================================================
# CLI Command Tests (Mocked)
# =============================================================================

class TestCLI:
    """Tests for CLI commands."""
    
    @pytest.fixture(autouse=True)
    def setup(self, temp_dir):
        self.temp_dir = temp_dir
        self.original_dir = os.getcwd()
        os.chdir(temp_dir)
    
    def teardown_method(self):
        os.chdir(self.original_dir)
    
    def test_cli_imports(self):
        """Test CLI functions can be imported."""
        from ralph_mode import (
            cmd_enable, cmd_disable, cmd_status,
            cmd_iterate, cmd_prompt, cmd_batch_init, cmd_next_task
        )
        
        assert callable(cmd_enable)
        assert callable(cmd_disable)
        assert callable(cmd_status)
        assert callable(cmd_iterate)
        assert callable(cmd_prompt)
        assert callable(cmd_batch_init)
        assert callable(cmd_next_task)


# =============================================================================
# Run Tests
# =============================================================================

if __name__ == '__main__':
    print(f"\nüîÑ Copilot Ralph Mode Advanced Test Suite v{VERSION}")
    print("=" * 60)
    
    # Check for hypothesis
    if HAS_HYPOTHESIS:
        print("‚úÖ Hypothesis available for property-based testing")
    else:
        print("‚ö†Ô∏è  Hypothesis not installed - property tests will be skipped")
        print("   Install with: pip install hypothesis")
    
    print()
    
    # Run with pytest
    sys.exit(pytest.main([__file__, '-v', '--tb=short']))
